{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from utils import yaml_config_hook\n",
    "\n",
    "from modules import SimCLR, LogisticRegression, get_resnet, EarlyStopping\n",
    "from modules.transformations import TransformsSimCLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"SimCLR\")\n",
    "config = yaml_config_hook(\"./config/config.yaml\")\n",
    "for k, v in config.items():\n",
    "    parser.add_argument(f\"--{k}\", default=v, type=type(v))\n",
    "\n",
    "args_str = '' \n",
    "args, _ = parser.parse_known_args(args=args_str)\n",
    "\n",
    "# args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args.device = torch.device('cuda')\n",
    "\n",
    "print(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "#------- added by young ---------\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "if args.gpus > 1:\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder(\n",
    "    '/home/opticho/source/SimCLR/datasets/dataset2(3)/train/train', \n",
    "    transform=TransformsSimCLR(size=(args.image_size, args.image_size)).test_transform)\n",
    "valid_dataset = torchvision.datasets.ImageFolder(\n",
    "    '/home/opticho/source/SimCLR/datasets/dataset2(3)/train/valid', \n",
    "    transform=TransformsSimCLR(size=(args.image_size, args.image_size)).test_transform)\n",
    "test_dataset = torchvision.datasets.ImageFolder(\n",
    "    '/home/opticho/source/SimCLR/datasets/dataset2(3)/test', \n",
    "    transform=TransformsSimCLR(size=(args.image_size, args.image_size)).test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 776\n",
       "    Root location: /home/opticho/source/SimCLR/datasets/dataset2(3)/test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(224, 224), interpolation=PIL.Image.BILINEAR)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "test_dataset # [ [ [image], [label] ] * 835 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.logistic_batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=args.workers,\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=args.logistic_batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=args.workers,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=args.logistic_batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=args.workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SimCLR(\n",
       "  (encoder): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (projector): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=2048, out_features=64, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "args.model_num = 5\n",
    "args.resnet = 'resnet50'\n",
    "args.batch_size = 32\n",
    "\n",
    "encoder = get_resnet(args.resnet, pretrained=False)\n",
    "n_features = encoder.fc.in_features\n",
    "\n",
    "simclr_model = SimCLR(args, encoder, n_features)\n",
    "model_fp = os.path.join(\n",
    "    args.model_path, \"model{}.tar\".format(args.model_num)\n",
    ")\n",
    "simclr_model.load_state_dict(torch.load(model_fp, map_location=args.device.type))\n",
    "simclr_model = simclr_model.to(args.device)\n",
    "simclr_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Linear(in_features=2048, out_features=3, bias=True)\n"
     ]
    }
   ],
   "source": [
    "n_classes = 3\n",
    "model = LogisticRegression(simclr_model.n_features, n_classes)\n",
    "model = model.to(args.device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2720, 675, 776)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "len(train_dataset), len(valid_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(loader, simclr_model, device):\n",
    "    feature_vector = []\n",
    "    labels_vector = []\n",
    "    for step, (x, y) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "\n",
    "        # get encoding\n",
    "        with torch.no_grad():\n",
    "            h, _, z, _ = simclr_model(x, x)\n",
    "\n",
    "        h = h.detach()\n",
    "\n",
    "        feature_vector.extend(h.cpu().detach().numpy())\n",
    "        labels_vector.extend(y.numpy())\n",
    "\n",
    "        # if step % 20 == 0:\n",
    "        #     print(f\"Step [{step}/{len(loader)}]\\t Computing features...\")\n",
    "\n",
    "    feature_vector = np.array(feature_vector)\n",
    "    labels_vector = np.array(labels_vector)\n",
    "    print(\"Features shape {}\".format(feature_vector.shape))\n",
    "    return feature_vector, labels_vector\n",
    "\n",
    "\n",
    "def get_features(simclr_model, train_loader, valid_loader, test_loader, device):\n",
    "    train_X, train_y = inference(train_loader, simclr_model, device)\n",
    "    valid_X, valid_y = inference(valid_loader, simclr_model, device)\n",
    "    test_X, test_y = inference(test_loader, simclr_model, device)\n",
    "    return train_X, train_y, valid_X, valid_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders_from_arrays(X_train, y_train, X_valid, y_valid, X_test, y_test, batch_size):\n",
    "    train = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(X_train), torch.from_numpy(y_train)\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    valid = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(X_valid), torch.from_numpy(y_valid)\n",
    "    )\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    \n",
    "    test = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(X_test), torch.from_numpy(y_test)\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    return train_loader, valid_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Features shape (2720, 2048)\n",
      "Features shape (675, 2048)\n",
      "Features shape (776, 2048)\n"
     ]
    }
   ],
   "source": [
    "(train_X, train_y, valid_X, valid_y, test_X, test_y) = get_features(\n",
    "    simclr_model, train_loader, valid_loader, test_loader, args.device\n",
    ")\n",
    "\n",
    "arr_train_loader, arr_valid_loader, arr_test_loader = create_data_loaders_from_arrays(\n",
    "    train_X, train_y, valid_X, valid_y, test_X, test_y, args.logistic_batch_size\n",
    ")\n",
    "patience = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(args, model, train_loader, valid_loader, criterion, optimizer, patience):\n",
    "    epochs = args.logistic_epochs\n",
    "    device = args.device\n",
    "    valid_loss_min = np.Inf\n",
    "    train_losses = []\n",
    "    # to track the validation loss as the model trains\n",
    "    valid_losses = []\n",
    "    # to track the average training loss per epoch as the model trains\n",
    "    avg_train_losses = []\n",
    "    # to track the average validation loss per epoch as the model trains\n",
    "    avg_valid_losses = [] \n",
    "    train_acc, valid_acc = [],[]\n",
    "    #valid_acc =[]\n",
    "    best_acc = 0.0\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "        total_train = 0\n",
    "        correct_train = 0\n",
    "        total_valid = 0\n",
    "        correct_valid = 0\n",
    "        \n",
    "        for step, (inputs, labels) in enumerate(train_loader):\n",
    "            \n",
    "            # Move input and label tensors to the default device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            predicted = outputs.argmax(1)\n",
    "            total_train += labels.nelement()\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            train_accuracy = correct_train / total_train\n",
    "        \n",
    "        model.eval()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            valid_accuracy = 0\n",
    "            for inputs, labels in valid_loader:\n",
    "                \n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                valid_losses.append(loss.item())\n",
    "                # Calculate accuracy\n",
    "                predicted = outputs.argmax(1)\n",
    "                total_valid += labels.nelement()\n",
    "                correct_valid += (predicted == labels).sum().item()\n",
    "                valid_accuracy = correct_valid / total_valid\n",
    "            \n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        valid_acc.append(valid_accuracy) \n",
    "        train_acc.append(train_accuracy)\n",
    "\n",
    "        # calculate average losses\n",
    "        \n",
    "        # print training/validation statistics \n",
    "        print(f\"Epoch {epoch+1}/{epochs}.. \")\n",
    "        #print('train Loss: {:.3f}'.format(epoch, loss.item()), \"Training Accuracy: %d %%\" % (train_accuracy))\n",
    "        #print('Training Accuracy: {:.6f}'.format(\n",
    "        #    train_accuracy))\n",
    "        print('Training Loss: {:.6f} \\tValidation Loss: {:.6f} \\tTraining Accuracy: {:.6f} \\tValidation Accuracy: {:.6f}'.format(\n",
    "            train_loss, valid_loss, train_accuracy*100, valid_accuracy*100))\n",
    "        train_losses = []\n",
    "        valid_losses = []        \n",
    "        if valid_accuracy > best_acc:\n",
    "            best_acc = valid_accuracy\n",
    "        early_stopping(valid_loss, args, model, optimizer, save=False)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        \n",
    "    print('Best val Acc: {:4f}'.format(best_acc*100))  \n",
    "    # model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    # plt.title(\"Accuracy vs. Number of Training Epochs\")\n",
    "    # plt.xlabel(\"Training Epochs\")\n",
    "    # plt.ylabel(\"Accuracy\")      \n",
    "    # plt.plot(train_acc, label='Training acc')\n",
    "    # plt.plot(valid_acc, label='Validation acc')\n",
    "    # plt.legend(frameon=False)\n",
    "    # plt.show()\n",
    "    return  model, avg_train_losses, avg_valid_losses,  train_acc, valid_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10000.. \n",
      "Training Loss: 0.852188 \tValidation Loss: 0.756382 \tTraining Accuracy: 0.000000 \tValidation Accuracy: 64.444444\n",
      "Epoch 2/10000.. \n",
      "Training Loss: 0.766821 \tValidation Loss: 0.749102 \tTraining Accuracy: 0.000000 \tValidation Accuracy: 64.296296\n",
      "Epoch 3/10000.. \n",
      "Training Loss: 0.742462 \tValidation Loss: 0.751043 \tTraining Accuracy: 0.000000 \tValidation Accuracy: 65.185185\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 4/10000.. \n",
      "Training Loss: 0.726065 \tValidation Loss: 0.754216 \tTraining Accuracy: 0.000000 \tValidation Accuracy: 65.037037\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 5/10000.. \n",
      "Training Loss: 0.713294 \tValidation Loss: 0.757160 \tTraining Accuracy: 0.000000 \tValidation Accuracy: 65.185185\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 6/10000.. \n",
      "Training Loss: 0.702615 \tValidation Loss: 0.759658 \tTraining Accuracy: 0.000000 \tValidation Accuracy: 65.185185\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 7/10000.. \n",
      "Training Loss: 0.693306 \tValidation Loss: 0.761758 \tTraining Accuracy: 0.000000 \tValidation Accuracy: 65.037037\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 8/10000.. \n",
      "Training Loss: 0.684978 \tValidation Loss: 0.763545 \tTraining Accuracy: 0.000000 \tValidation Accuracy: 64.592593\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 9/10000.. \n",
      "Training Loss: 0.677391 \tValidation Loss: 0.765090 \tTraining Accuracy: 0.000000 \tValidation Accuracy: 64.740741\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 10/10000.. \n",
      "Training Loss: 0.670391 \tValidation Loss: 0.766449 \tTraining Accuracy: 0.000000 \tValidation Accuracy: 64.592593\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 11/10000.. \n",
      "Training Loss: 0.663869 \tValidation Loss: 0.767660 \tTraining Accuracy: 0.000000 \tValidation Accuracy: 64.444444\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 12/10000.. \n",
      "Training Loss: 0.657749 \tValidation Loss: 0.768752 \tTraining Accuracy: 0.000000 \tValidation Accuracy: 63.407407\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 13/10000.. \n",
      "Training Loss: 0.651971 \tValidation Loss: 0.769745 \tTraining Accuracy: 0.000000 \tValidation Accuracy: 63.111111\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 14/10000.. \n",
      "Training Loss: 0.646491 \tValidation Loss: 0.770656 \tTraining Accuracy: 0.000000 \tValidation Accuracy: 63.407407\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 15/10000.. \n",
      "Training Loss: 0.641272 \tValidation Loss: 0.771497 \tTraining Accuracy: 0.000000 \tValidation Accuracy: 63.555556\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 16/10000.. \n",
      "Training Loss: 0.636287 \tValidation Loss: 0.772279 \tTraining Accuracy: 0.000000 \tValidation Accuracy: 63.111111\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 17/10000.. \n",
      "Training Loss: 0.631511 \tValidation Loss: 0.773009 \tTraining Accuracy: 0.000000 \tValidation Accuracy: 63.259259\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Epoch 18/10000.. \n",
      "Training Loss: 0.626924 \tValidation Loss: 0.773694 \tTraining Accuracy: 0.000000 \tValidation Accuracy: 63.111111\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Epoch 19/10000.. \n",
      "Training Loss: 0.622509 \tValidation Loss: 0.774340 \tTraining Accuracy: 0.000000 \tValidation Accuracy: 63.111111\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Epoch 20/10000.. \n",
      "Training Loss: 0.618253 \tValidation Loss: 0.774953 \tTraining Accuracy: 0.000000 \tValidation Accuracy: 63.111111\n",
      "EarlyStopping counter: 18 out of 20\n",
      "Epoch 21/10000.. \n",
      "Training Loss: 0.614141 \tValidation Loss: 0.775535 \tTraining Accuracy: 0.000000 \tValidation Accuracy: 63.111111\n",
      "EarlyStopping counter: 19 out of 20\n",
      "Epoch 22/10000.. \n",
      "Training Loss: 0.610164 \tValidation Loss: 0.776091 \tTraining Accuracy: 0.000000 \tValidation Accuracy: 63.259259\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n",
      "Best val Acc: 65.185185\n"
     ]
    }
   ],
   "source": [
    "args.logistic_epochs = 10000\n",
    "\n",
    "model, train_loss, valid_loss, train_acc, valid_acc = train_model(\n",
    "    args, model, \n",
    "    arr_train_loader, arr_valid_loader, \n",
    "    criterion, optimizer, patience)\n",
    "\n",
    "if not os.path.exists(args.model_path):\n",
    "    os.makedirs(args.model_path)\n",
    "\n",
    "out = os.path.join(args.model_path, \"downstream_model5.tar\")\n",
    "\n",
    "torch.save(model.state_dict(), out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, loader, model, criterion, optimizer):\n",
    "    loss_epoch = 0\n",
    "    accuracy_epoch = 0\n",
    "    model.eval()\n",
    "    pred = []\n",
    "    true = []\n",
    "    soft = []\n",
    "    for step, (x, y) in enumerate(loader):\n",
    "        model.zero_grad()\n",
    "\n",
    "        x = x.to(args.device)\n",
    "        y = y.to(args.device)\n",
    "\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        \n",
    "        # for majority voting\n",
    "        softmax = torch.nn.Softmax(dim=1)\n",
    "        s = softmax(outputs).cpu().detach().tolist()\n",
    "        for i in range(len(s)):\n",
    "            soft.append(s[i])\n",
    "\n",
    "        predicted = outputs.argmax(1)\n",
    "        preds = predicted.cpu().numpy()\n",
    "        labels = y.cpu().numpy()\n",
    "        preds = np.reshape(preds, (len(preds), 1))\n",
    "        labels = np.reshape(labels, (len(preds), 1))\n",
    "\n",
    "        for i in range(len(preds)):\n",
    "            pred.append(preds[i][0].item())\n",
    "            true.append(labels[i][0].item())\n",
    "        \n",
    "        acc = (predicted == y).sum().item() / y.size(0)\n",
    "        accuracy_epoch += acc\n",
    "\n",
    "        loss_epoch += loss.item()\n",
    "\n",
    "    cnf_matrix = confusion_matrix(true, pred)\n",
    "    print('Confusion Matrix:\\n', cnf_matrix)\n",
    "\n",
    "    FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n",
    "    FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "    TP = np.diag(cnf_matrix)\n",
    "    TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "    FP = FP.astype(float)\n",
    "    FN = FN.astype(float)\n",
    "    TP = TP.astype(float)\n",
    "    TN = TN.astype(float)\n",
    "\n",
    "    accuracy_epoch = np.diag(cnf_matrix).sum().item() / len(true)\n",
    "    \n",
    "    # Specificity or true negative rate\n",
    "    specificity = TN/(TN+FP) \n",
    "\n",
    "    print_specificity(specificity)\n",
    "\n",
    "    report = classification_report(true, pred, target_names=['covid', 'healthy', 'others'])\n",
    "    print(report)\n",
    "\n",
    "    return loss_epoch, accuracy_epoch, (pred, true, soft)\n",
    "\n",
    "def print_specificity(specificity):\n",
    "    print('\\t\\tspecificity')\n",
    "    print('')\n",
    "\n",
    "    print(f'       covid\\t{specificity[0]:.2f}')\n",
    "    print(f'     healthy\\t{specificity[1]:.2f}')\n",
    "    print(f'      others\\t{specificity[2]:.2f}')\n",
    "    print('')\n",
    "\n",
    "    macro_specificity = sum(specificity) / 3.0\n",
    "    print(f'   macro avg\\t{macro_specificity:.2f}')\n",
    "\n",
    "    weighted = [434/835, 152/835, 249/835] \n",
    "    weighted_specificity = weighted @ specificity\n",
    "    print(f'weighted avg\\t{weighted_specificity:.2f}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Confusion Matrix:\n [[342   9  51]\n [ 24  92  17]\n [155  30  56]]\n\t\tspecificity\n\n       covid\t0.52\n     healthy\t0.94\n      others\t0.87\n\n   macro avg\t0.78\nweighted avg\t0.70\n\n              precision    recall  f1-score   support\n\n       covid       0.66      0.85      0.74       402\n     healthy       0.70      0.69      0.70       133\n      others       0.45      0.23      0.31       241\n\n    accuracy                           0.63       776\n   macro avg       0.60      0.59      0.58       776\nweighted avg       0.60      0.63      0.60       776\n\n[FINAL]\t Loss: 0.7204628729820252\t Accuracy: 0.6314432989690721\n"
     ]
    }
   ],
   "source": [
    "# final testing\n",
    "loss_epoch, accuracy_epoch, result = test(\n",
    "    args, arr_test_loader, model, criterion, optimizer\n",
    ")\n",
    "print(\n",
    "    f\"[FINAL]\\t Loss: {loss_epoch / len(arr_test_loader)}\\t Accuracy: {accuracy_epoch}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "preds, true, soft = result\n",
    "images_path = test_loader.dataset.samples\n",
    "# images_path -> [ [images path, label] * 835 ]\n",
    "\n",
    "with open(f\"majority{args.model_num}.csv\", \"w\") as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow([\"file\", \"prob_0\", \"prob_1\", \"prob_2\", \"pred\", \"label\"])\n",
    "    for i in range(len(preds)):\n",
    "        f = os.path.basename(images_path[i][0])\n",
    "        prob_0 = round(soft[i][0], 6)\n",
    "        prob_1 = round(soft[i][1], 6)\n",
    "        prob_2 = round(soft[i][2], 6)\n",
    "        pred = preds[i]\n",
    "        label = true[i]\n",
    "        wr.writerow([f, prob_0, prob_1, prob_2, pred, label])    "
   ]
  }
 ]
}